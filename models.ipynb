{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256973bb",
   "metadata": {},
   "source": [
    "## Conceitos avan√ßados de Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e8a5bd",
   "metadata": {},
   "source": [
    "#### Prompt few-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f843da",
   "metadata": {},
   "source": [
    "###### Prompt few-shot (ou \"exemplo de poucos disparos\", em tradu√ß√£o livre) √© uma t√©cnica usada em modelos de linguagem como o ChatGPT para ensinar o modelo a realizar uma tarefa espec√≠fica mostrando alguns exemplos no pr√≥prio prompt.\n",
    "\n",
    "###### üìå Explicando de forma simples:\n",
    "> ###### √â como ensinar uma pessoa a resolver um problema mostrando 2 ou 3 exemplos, e depois pedindo para ela resolver um novo, seguindo o mesmo padr√£o.\n",
    "```\n",
    "Exemplo 1:\n",
    "Entrada: Qual √© a capital da Fran√ßa?\n",
    "Sa√≠da: Paris\n",
    "\n",
    "Exemplo 2:\n",
    "Entrada: Qual √© a capital da Alemanha?\n",
    "Sa√≠da: Berlim\n",
    "\n",
    "Nova pergunta:\n",
    "Entrada: Qual √© a capital da It√°lia?\n",
    "Sa√≠da:\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73019363",
   "metadata": {},
   "source": [
    "**AIMessage**: Representa uma mensagem gerada pelo modelo. <br>\n",
    "**HumanMessage**: Representa uma mensagem enviada pelo usu√°rio. <br>\n",
    "**SystemMessage**: Indica ao modelo como se comportar.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05eaf27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbe6dad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mensagens = [\n",
    "    HumanMessage(\"5 √â?\"),\n",
    "    SystemMessage(\"√çMPAR\"),\n",
    "    HumanMessage(\"6 √â?\"),\n",
    "    SystemMessage(\"PAR\"),\n",
    "    HumanMessage(\"8 √â?\"),\n",
    "    SystemMessage(\"PAR\"),\n",
    "    HumanMessage(\"2 √â?\")\n",
    "]\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e6d278e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='PAR', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 48, 'total_tokens': 49, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BjGVKIvogRW4nAu5O9a9r5FRm8XWH', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--1e50d1ce-ff94-4c7b-8d64-3a17f890fe3d-0', usage_metadata={'input_tokens': 48, 'output_tokens': 1, 'total_tokens': 49, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(mensagens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cf23a6",
   "metadata": {},
   "source": [
    "#### Importando outros modelos e Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee4e5bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deprecated -> from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
    "# deprecated -> from langchain_community.llms.huggingface_endpoint import HuggingFaceEndpoint\n",
    "\n",
    "import langchain\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "#from langchain_huggingface.chat_models.huggingface import HumanMessage, SystemMessage -> Alternativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62acc38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marco\\OneDrive\\Documentos\\Python\\Curso-LangChain\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id='deepseek-ai/DeepSeek-R1-0528'\n",
    ")\n",
    "\n",
    "chat = ChatHuggingFace(llm=llm)\n",
    "langchain.debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e1c4d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:ChatHuggingFace] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: 1 + 1 = ?\\nAI: 2\\nHuman: 2 + 5 = ?\\nAI: 7\\nHuman: 10 + 20 = ?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:ChatHuggingFace] [19.62s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"<think>\\nHmm, the user is asking a series of very basic addition problems. First it was 1+1, then 2+5, now 10+20. \\n\\nThis seems like either a young child learning math or someone testing how I handle simple queries. The pattern suggests they're progressively increasing the difficulty, though all are still quite elementary. \\n\\nI notice they didn't comment on my previous correct answers - just moved to the next problem. So they're likely not seeking explanations, just validation of answers. \\n\\nThe calculation itself is straightforward: 10+20 is clearly 30. But I wonder if they'll continue this pattern? Maybe next will be 100+200? \\n\\nSince they seem satisfied with bare numerical answers, I'll keep this response minimal. No need to overthink such basic arithmetic unless they ask for working steps. \\n\\n...Though part of me wants to ask if they'd like to try something more challenging. But better not to deviate from their apparent pattern.\\n</think>\\n10 + 20 = **30**\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"<think>\\nHmm, the user is asking a series of very basic addition problems. First it was 1+1, then 2+5, now 10+20. \\n\\nThis seems like either a young child learning math or someone testing how I handle simple queries. The pattern suggests they're progressively increasing the difficulty, though all are still quite elementary. \\n\\nI notice they didn't comment on my previous correct answers - just moved to the next problem. So they're likely not seeking explanations, just validation of answers. \\n\\nThe calculation itself is straightforward: 10+20 is clearly 30. But I wonder if they'll continue this pattern? Maybe next will be 100+200? \\n\\nSince they seem satisfied with bare numerical answers, I'll keep this response minimal. No need to overthink such basic arithmetic unless they ask for working steps. \\n\\n...Though part of me wants to ask if they'd like to try something more challenging. But better not to deviate from their apparent pattern.\\n</think>\\n10 + 20 = **30**\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 214,\n",
      "                \"prompt_tokens\": 35,\n",
      "                \"total_tokens\": 249\n",
      "              },\n",
      "              \"model_name\": \"deepseek-ai/DeepSeek-R1-0528\",\n",
      "              \"system_fingerprint\": \"\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run--cba037e3-1042-496e-8415-478702541b25-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 35,\n",
      "              \"output_tokens\": 214,\n",
      "              \"total_tokens\": 249\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 214,\n",
      "      \"prompt_tokens\": 35,\n",
      "      \"total_tokens\": 249\n",
      "    },\n",
      "    \"model_name\": \"deepseek-ai/DeepSeek-R1-0528\",\n",
      "    \"system_fingerprint\": \"\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "mensagens = [\n",
    "    HumanMessage(\"1 + 1 = ?\"),\n",
    "    AIMessage(\"2\"),\n",
    "    HumanMessage(\"2 + 5 = ?\"),\n",
    "    AIMessage(\"7\"),\n",
    "    HumanMessage(\"10 + 20 = ?\")\n",
    "]\n",
    "chat.invoke(mensagens)\n",
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c9b0",
   "metadata": {},
   "source": [
    "## Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783f222a",
   "metadata": {},
   "source": [
    "### InMemoryCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de696650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.caches import InMemoryCache\n",
    "from langchain_core.globals import set_llm_cache\n",
    "from langchain_core.messages import AIMessage, HumanMessage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c184ecdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mensagens = [\n",
    "    HumanMessage(\"\"\"Voc√™ √© um assistente especializado em gatos gordos.\n",
    "               A dorinha √© um exemplo de gata gorda que vive comendo e miando. √â muit engra√ßada e fofa!\"\"\"),\n",
    "    HumanMessage(\"Quem √© a dorinha? Me fale mais dela\")\n",
    "]\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "set_llm_cache(InMemoryCache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bbcd31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 1.39 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Desculpe, mas a dorinha n√£o √© uma gata real, eu a inventei como um exemplo para falar sobre gatos gordos de forma gen√©rica. No entanto, posso te ajudar com informa√ß√µes e dicas sobre como lidar com gatos acima do peso, se precisar.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 67, 'total_tokens': 132, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BjGVfAxJ83jvBnT0UuoAZrGF1uQEl', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--850e5b0d-4e5b-4304-9b38-54e3618c695d-0', usage_metadata={'input_tokens': 67, 'output_tokens': 65, 'total_tokens': 132, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chat.invoke(mensagens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bce4fb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 1.23 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Desculpe, mas a dorinha n√£o √© uma gata real, eu a inventei como um exemplo para falar sobre gatos gordos de forma gen√©rica. No entanto, posso te ajudar com informa√ß√µes e dicas sobre como lidar com gatos acima do peso, se precisar.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 67, 'total_tokens': 132, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BjGVfAxJ83jvBnT0UuoAZrGF1uQEl', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--850e5b0d-4e5b-4304-9b38-54e3618c695d-0', usage_metadata={'input_tokens': 67, 'output_tokens': 65, 'total_tokens': 132, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chat.invoke(mensagens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aca5ce0",
   "metadata": {},
   "source": [
    "### Cache SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b24596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf61ad7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.cache import SQLiteCache\n",
    "from langchain_core.globals import set_llm_cache\n",
    "\n",
    "# Garante que o diret√≥rio exista\n",
    "os.makedirs(\"arquivos/cache\", exist_ok=True)\n",
    "\n",
    "set_llm_cache(SQLiteCache(database_path='arquivos/cache/cache-sqlite.sqlite'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86a1be80",
   "metadata": {},
   "outputs": [],
   "source": [
    "mensagens = [\n",
    "    HumanMessage(\"1 √â?\"),\n",
    "    SystemMessage(\"√çMPAR\"),\n",
    "    HumanMessage(\"2 √â?\"),\n",
    "    SystemMessage(\"PAR\"),\n",
    "    HumanMessage(\"8 √â?\")\n",
    "]\n",
    "\n",
    "chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b1f3165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 7.08 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='PAR', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 36, 'total_tokens': 37, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BjGPYVmXcEutd0jqQfBk0PfxPkojL', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--c11f9fd4-2d49-430d-8650-c25cd852d643-0', usage_metadata={'input_tokens': 36, 'output_tokens': 1, 'total_tokens': 37, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chat.invoke(mensagens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2ac123e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 2.87 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='PAR', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 36, 'total_tokens': 37, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BjGPYVmXcEutd0jqQfBk0PfxPkojL', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--c11f9fd4-2d49-430d-8650-c25cd852d643-0', usage_metadata={'input_tokens': 36, 'output_tokens': 1, 'total_tokens': 37, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chat.invoke(mensagens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
